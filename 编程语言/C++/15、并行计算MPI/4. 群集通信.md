# 广播
```c++
int MPI_Bcast(
	void *buffer,              // 缓冲
	int count,                 // 数量
	MPI_Datatype type,         // 类型
	int root,                  // 根节点
	MPI_Comm comm              // 通信域
);

if (rank == 0) {
	for (int i = 0; i < 10; i++)
		a[i] = i;
	MPI_Bcast(a, 10, MPI_INT, 0, MPI_COMM_WORLD);
} else {
	MPI_Bcast(a, 10, MPI_INT, 0, MPI_COMM_WORLD);
	printf("P%d: ", rank);
	for (int i = 0; i < 10; i++)
		printf("%d ", a[i]);
	printf("\n");
}
```
1. 当进程号和根节点号相同时是发送信息
2. 当进程号和根节点不同时是接受信息

# 散播
根进程将连续的数据块分成等长的子块，并按照rank的顺序分发给通信域中的所有进程

```c++
int MPI_Scatter(
	const void *sendbuf,
	int sendcount,            // 每个块中的元素个数
	MPI_Datatype,
	void *recvbuf,
	int recvcount,
	MPI_Datatype,
	int root,
	MPI_Comm com
)
```
1. 根进程：自身保留`a[0] ~ a[9]`
2. 其他进程：进程1接受`a[10] ~ a[19]`，以此类推

# 收集
将所有进程的数据按rank顺序收集到根进程中

```c++
int MPI_Gather(
    const void* sendbuf,  // 所有进程的发送缓冲区
    int sendcount,        // 单个进程发送的元素数量
    MPI_Datatype sendtype,
    void* recvbuf,        // 根进程的接收缓冲区（其他进程设为NULL）
    int recvcount,        // **每个子块**接收的元素数量（通常等于sendcount）
    MPI_Datatype recvtype,
    int root,             // 根进程rank
    MPI_Comm comm
);

if (rank == 0) {
	int a[40];
	MPI_Gather(b, 10, MPI_INT, a, 10, MPI_INT, 0, MPI_COMM_WORLD);
	for (int i = 0; i < 40; i++)
		printf("%d ", a[i]);
} else
	MPI_Gather(b, 10, MPI_INT, NULL, 10, MPI_INT, 0, MPI_COMM_WORLD);
```

# 全局收集
和收集很类似，但是每一个进程都可以接受全局的数据

```c++
int MPI_Allgather(
    const void* sendbuf,  // 各进程的发送数据块（如您的 `b[10]`）
    int sendcount,        // 单个进程发送的元素数量
    MPI_Datatype sendtype,
    void* recvbuf,        // 全量数据接收缓冲区（如 `a[40]`）
    int recvcount,        // **单个进程发送的元素数量**（需=sendcount）
    MPI_Datatype recvtype,
    MPI_Comm comm
);
```
1. 其他进程recvbuf不会设置为null

# 聚合功能

## 归约
各个进程按照元素进行聚合操作

```c++
int MPI_Reduce(
    const void* sendbuf,  // 各进程的输入数据指针（如您的 `a`）
    void* recvbuf,        // 根进程的输出缓冲区（如 `b`，非根进程设为 `NULL`）
    int count,            // **每个进程的输入元素数量**（如4）
    MPI_Datatype datatype,// 数据类型（如 `MPI_INT`）
    MPI_Op op,            // 预定义聚合操作（如 `MPI_SUM`）
    int root,             // 根进程rank（如0）
    MPI_Comm comm         // 通信域
);

for (int i = 0; i < 4; i++)
	a[i] = i * rank;
MPI_Reduce(a, b, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

// 每一个元素独立累加，最终得到如下结果
b[0] = 0+0+0+0 = 0  
b[1] = 0+1+2+3 = 6  
b[2] = 0+2+4+6 = 12  
b[3] = 0+3+6+9 = 18
```
## 扫描
扫描是一种前缀聚合操作

```c++
for (int i = 0; i < 4; i++)
	a[i] = i * rank;
MPI_Scan(a, c, 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);

// 最终每个进程的输出结果
P0: 0 0 0 0        # rank0 的 c[4]
P1: 0 1 2 3        # rank1 的 c[4]
P2: 0 3 6 9        # rank2 的 c[4]
P3: 0 6 12 18      # rank3 的 c[4]
```

